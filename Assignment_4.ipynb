{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1aa118",
   "metadata": {},
   "source": [
    "# Scraping specific information from detail pages using Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc7a74",
   "metadata": {},
   "source": [
    "The objective of this section is to scrape specific information from the detail pages of all Chinese novels on the Wuxia World website. The key information includes the title, author, genres, rating, number of chapters, number of reviews, and details of the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226e0c8",
   "metadata": {},
   "source": [
    "## 1.Installing Selenium and Browser Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be238c19",
   "metadata": {},
   "source": [
    "In this case, we chose to use Selenium because the Wuxia World website has implemented an anti-scraping mechanism that requires browser simulation to obtain complete data. For installing and documentation, please refer to https://selenium-python.readthedocs.io/index.html\n",
    "\n",
    "Note: Please make sure to install the necessary libraries such as Selenium and BeautifulSoup before running the code. You can use pip, the Python package installer, to install these libraries. To install them, open your command prompt or terminal and enter the following commands:\n",
    "\n",
    "pip install selenium;\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "69ad2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from lxml import etree\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8220ac",
   "metadata": {},
   "source": [
    "Here, we take ChromeDriver as an example. Please follow the setup instruction https://sites.google.com/chromium.org/driver/getting-started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed9205",
   "metadata": {},
   "source": [
    "## 2. Analyzing the webpage structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f91a2d",
   "metadata": {},
   "source": [
    "Firstly, we need to browse the page structure of the target website and examine the web pages to determine their types.\n",
    "\n",
    "Based on observation, we have noticed that all the Chinese novels are listed in alphabetical order in a table on the webpage. Each target novel's title corresponds to a hyperlink that leads to its detail page. In the table, only partial information, such as the novel's name and rating, is displayed. To access more detailed information, one needs to click on the hyperlink. Therefore, our overall web scraping approach is as follows:\n",
    "\n",
    "(1).Obtain a complete list of links corresponding to all the novels;\n",
    "(2).Iterate through the target link list to scrape specific information from each webpage and store it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a17f6",
   "metadata": {},
   "source": [
    "## 3. Obtaining the URL list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d480d5",
   "metadata": {},
   "source": [
    "Using the developer tool, we discovered that the URL is contained within the \"href\" attribute of the novel's title. To obtain a complete list of URLs for the novels, we will follow the steps of locating the novel's title and extracting the \"href\" attribute value of the element.\n",
    "\n",
    "Selenium provides various methods for locating elements, such as XPath, ID, and name. In this case, we will use XPath to locate the element. Once we have obtained the XPath of the corresponding element, we will use the syntax \"@href\" to select the element's attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df16997b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-90-c656d770cb62>:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path='/Users/wanshuo/Desktop/Master/DH_MA_thesis/Dataset/chromedriver_mac_arm64/chromedriver')\n"
     ]
    }
   ],
   "source": [
    "# Specify the path of the chromedriver and launch the browser\n",
    "driver = webdriver.Chrome(executable_path='/Users/wanshuo/Desktop/Master/DH_MA_thesis/Dataset/chromedriver_mac_arm64/chromedriver')\n",
    "# Navigate to the URL of wuxiaworld\n",
    "driver.get('https://www.wuxiaworld.com/novels')\n",
    "\n",
    "# Wait for 10 seconds for the page to load\n",
    "time.sleep(10)\n",
    "    \n",
    "# Click the 'Chinese' checkbox to show only Chinese novels\n",
    "driver.find_element(By.XPATH,'//*[@id=\"loading-container-replacement\"]/div/div[1]/div[2]/div/div/div/div[1]/div/label[2]/span[2]').click()\n",
    "\n",
    "# Wait for 10 seconds for the page to load\n",
    "time.sleep(10)\n",
    "    \n",
    "# Scroll down the page multiple times to load all the novel data\n",
    "for i in range(60):\n",
    "    # Simulate the Page Down key press using ActionChains\n",
    "    ActionChains(driver).key_down(Keys.PAGE_DOWN).key_up(Keys.PAGE_DOWN).perform()\n",
    "    # Wait for a short time for the page to load\n",
    "    time.sleep(0.2)\n",
    "        \n",
    "# Create an empty list to store all Chinese novels URLs\n",
    "all_url_list = []\n",
    "\n",
    "# Xpath locate the element\n",
    "url_xpath = '//*[@id=\"loading-container-replacement\"]/div/div[2]/div/div/div/div/div/div/div/div/div[2]/p/a/@href'\n",
    "\n",
    "# Get the HTML source of the page\n",
    "html = driver.page_source\n",
    "# Parse the HTML source using the etree.HTML function\n",
    "tree = etree.HTML(html)\n",
    "# Extract the list of URLs using the XPath expression\n",
    "url_list = tree.xpath(url_xpath)\n",
    "# Extend the all_url_list with the extracted URL list\n",
    "all_url_list.extend(url_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a090e",
   "metadata": {},
   "source": [
    "## 4. Iterating through the URL list to scrape specific information from the detail pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911dcb2c",
   "metadata": {},
   "source": [
    "After obtaining the complete URL list of the novels, we need to iterate through the list to retrieve the details page of each novel. On this page, the key content we need to scrape includes:\n",
    "\n",
    "(1)Title;\n",
    "(2)Author;\n",
    "(3)Genres;\n",
    "(4)Rating;\n",
    "(5)Number of chapters;\n",
    "(6)Number of reviews;\n",
    "(7)Details of the reviews.\n",
    "\n",
    "Please note that the above content is the crucial information we need to extract from the details page of each novel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5738edf",
   "metadata": {},
   "source": [
    "### 4.1 Scraping the Title, Author, Genres, Rating, Number of chapters, and Number of reviews from the novel pages corresponding to each URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe5dd4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-74-54f9fccd8d19>:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path='/Users/wanshuo/Desktop/Master/DH_MA_thesis/Dataset/chromedriver_mac_arm64/chromedriver', options=CHROME_OPTIONS)\n"
     ]
    }
   ],
   "source": [
    "# Browser driver configuration and usage\n",
    "# Create an instance of ChromeOptions to customize Chrome browser settings\n",
    "CHROME_OPTIONS = webdriver.ChromeOptions()\n",
    "# Specify the preference for displaying images\n",
    "# 1 for displaying images, 2 for not displaying images. When images are not needed to be crawled, they can be set to not load images to save time.\n",
    "prefs = {\"profile.managed_default_content_settings.images\":2}   \n",
    "# Add the image display preference to the Chrome options\n",
    "CHROME_OPTIONS.add_experimental_option(\"prefs\", prefs)\n",
    "# Create a Chrome WebDriver instance with the specified driver executable path and options\n",
    "driver = webdriver.Chrome(executable_path='/Users/wanshuo/Desktop/Master/DH_MA_thesis/Dataset/chromedriver_mac_arm64/chromedriver', options=CHROME_OPTIONS)\n",
    "\n",
    "# Xpath locating for basic novel information\n",
    "title_xpath = '//*[@id=\"loading-container-replacement\"]/div/div[1]/div/div/div[2]/div[1]/div[2]/h1'\n",
    "author_xpath = '//*[@id=\"loading-container-replacement\"]/div/div[1]/div/div/div[2]/div[3]/div[1]/div[2]'\n",
    "genres_xpath = '//*[@id=\"full-width-tabpanel-0\"]/div/div[1]/div[2]/div/a/div/div'\n",
    "rating_xpath = '//*[@id=\"loading-container-replacement\"]/div/div[1]/div/div/div[2]/div[2]/div/span/span'\n",
    "chapters_xpath = '//*[@id=\"full-width-tabpanel-0\"]/div/div[1]/div[1]/div[1]/div[2]'\n",
    "reviews_xpath = '//*[@id=\"loading-container-replacement\"]/div/div[1]/div/div/div[2]/div[2]/div/div/span'\n",
    "\n",
    "# Create an empty list to store all the titleï¼Œauthor,genres,rating,chapters and reviews \n",
    "all_title = []\n",
    "all_author = []\n",
    "all_genres = []\n",
    "all_rating = []\n",
    "all_chapters = []\n",
    "all_reviews = []\n",
    "\n",
    "# Iterate through the list of URLs\n",
    "for url_item in all_url_list:\n",
    "    url = 'https://www.wuxiaworld.com' + url_item\n",
    "    driver.get(url) \n",
    "    time.sleep(5)\n",
    "    # Parse the webpage\n",
    "    html = driver.page_source\n",
    "    tree = etree.HTML(html)\n",
    "    \n",
    "    # Extract the text nodes of novel basic information\n",
    "    title_list = tree.xpath(title_xpath + '/text()')\n",
    "    all_title.append(title_list[0])\n",
    "    \n",
    "    author_list = tree.xpath(author_xpath + '/text()')\n",
    "    all_author.append(author_list[0])\n",
    "    \n",
    "    genres_list = tree.xpath(genres_xpath+ '/text()')\n",
    "    content = ';'.join(genres_list)  # There may be multiple genres, concatenate them with a semicolon\n",
    "    all_genres.append(content)\n",
    "    \n",
    "    rating_list = tree.xpath(rating_xpath + '/text()')\n",
    "    all_rating.append(rating_list[0])\n",
    "    \n",
    "    chapters_list = tree.xpath(chapters_xpath + '/text()')\n",
    "    all_chapters.append(chapters_list[0])\n",
    "    \n",
    "    reviews_list = tree.xpath(reviews_xpath + '/text()')\n",
    "    all_reviews.append(reviews_list[0])\n",
    "\n",
    "# Specify the file path and open the CSV file in write mode with UTF-8 encoding    \n",
    "with open('data_list.csv', 'w',encoding='utf-8',newline='') as csvfile:\n",
    "    # Create a CSV writer object\n",
    "    writer = csv.writer(csvfile)\n",
    "    # Write the header row with column names\n",
    "    writer.writerow(['title','author','genres','rating','chapters','reviews','url'])\n",
    "    # Iterate through each URL and corresponding data, and write them as rows in the CSV file\n",
    "    for url_item, row in zip(all_url_list, zip(all_title, all_author, all_genres, all_rating, all_chapters, all_reviews)):\n",
    "        # Convert the row elements to a list and append the URL item to the end\n",
    "        writer.writerow(list(row) + [f'https://www.wuxiaworld.com{url_item}'])\n",
    "\n",
    "# Close the browser driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c79c04",
   "metadata": {},
   "source": [
    "### 4.2 Scraping the details of the reviews from the novel pages corresponding to each URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "465226b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-92-6a0aa88beaf5>:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path='/Users/wanshuo/Desktop/Master/DH_MA_thesis/Dataset/chromedriver_mac_arm64/chromedriver')\n"
     ]
    }
   ],
   "source": [
    "# Specify the path of the chromedriver and launch the browser\n",
    "driver = webdriver.Chrome(executable_path='/Users/wanshuo/Desktop/Master/DH_MA_thesis/Dataset/chromedriver_mac_arm64/chromedriver')\n",
    "# Open the novel page\n",
    "driver.get('https://www.wuxiaworld.com/novel/rmji')\n",
    "\n",
    "# Wait for 10 seconds for the page to load\n",
    "time.sleep(10)\n",
    "\n",
    "# Replace the generic view_all path\n",
    "driver.find_element(By.XPATH,'//*[@id=\"full-width-tabpanel-0\"]/div/div[3]/div[2]/div[2]/div/div[2]/div/span').click()\n",
    "\n",
    "# Wait for 10 seconds for the page to load\n",
    "time.sleep(10)\n",
    "\n",
    "# Create an empty list to store all the detail reviews\n",
    "detail_reviews_list= []\n",
    "\n",
    "# Get the reviews on the first page\n",
    "html = driver.page_source # Get the HTML source code of the current page\n",
    "soup = BeautifulSoup(html, \"html.parser\") # Create a BeautifulSoup object to parse the HTML\n",
    "data = soup.find_all('div', class_=\"absolute top-0 -z-10 line-clamp-1 font-set-r15-h150 text-gray-t1 sm2:font-set-r16-h150\")\n",
    "# Find all the review elements with the specified class\n",
    "# Note: The class represents the CSS styles applied to the review elements\n",
    "\n",
    "data = data[3:] # Remove the first three reviews from the list of review elements\n",
    "detail_reviews_list.extend(data) # Add the remaining reviews to the detail_reviews_list\n",
    "\n",
    "# Use a set to keep track of already retrieved reviews\n",
    "seen_reviews = set([r.get_text(strip=True) for r in detail_reviews_list])\n",
    "\n",
    "# Get the reviews from the next page\n",
    "next_page=driver.find_element(By.XPATH,'/html/body/div[2]/div[3]/div/div/div/div[2]/div[3]/nav/ul/li[last()-0]/button')\n",
    "\n",
    "# Keep clicking the next page button until it's disabled\n",
    "while next_page.is_enabled():\n",
    "    next_page.click()\n",
    "    time.sleep(5)\n",
    "    html = driver.page_source # Get the HTML source code of the current page\n",
    "    soup = BeautifulSoup(html, \"html.parser\") # Create a BeautifulSoup object to parse the HTML\n",
    "    data = soup.find_all('div', class_=\"absolute top-0 -z-10 line-clamp-1 font-set-r15-h150 text-gray-t1 sm2:font-set-r16-h150\")\n",
    "    # Find all the review elements with the specified class\n",
    "    # Note: The class represents the CSS styles applied to the review elements\n",
    "    \n",
    "    # Add the unseen reviews to the detail_reviews_list\n",
    "    for review in data:\n",
    "        if review.get_text(strip=True) not in seen_reviews:\n",
    "            detail_reviews_list.append(review)\n",
    "            seen_reviews.add(review.get_text(strip=True))\n",
    "    # Find the next page button and wait until it is present\n",
    "    next_page = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH,'/html/body/div[2]/div[3]/div/div/div/div[2]/div[3]/nav/ul/li[last()-0]/button')))\n",
    "\n",
    "# Open a text file named 'detail_reviews.txt' in write mode, using UTF-8 encoding\n",
    "with open('detail_reviews.txt', 'w', encoding='utf-8') as f:\n",
    "    # Iterate through each detail_review in the detail_reviews_list\n",
    "    for detail_review in detail_reviews_list:\n",
    "        # Write the stripped text of the detail_review to the file\n",
    "        f.write(detail_review.get_text(strip=True))\n",
    "        # Write two newline characters to create a blank line between reviews\n",
    "        f.write('\\n\\n')\n",
    "\n",
    "# Close the browser driver        \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee84931",
   "metadata": {},
   "source": [
    "# Installing, Importing and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5934a044",
   "metadata": {},
   "source": [
    "Due to the huge amount of data (about 9000 reviews), the loading time is too long, so the reviews of the top 5 novels in rating (about 500 reviews) were chosen as the corpus in this assignment to improve the efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efcc7f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from langdetect) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the langdetect library\n",
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c498317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: jinja2 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (1.10.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Install the spaCy library\n",
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "904e71ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f7616e",
   "metadata": {},
   "source": [
    "In the preprocessing stage, keep only English reviews, remove non-letter, number, space and punctuation special characters. Additionally, all reviews were converted to lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f7693e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path containing the files\n",
    "corpora_folder_path = '/Users/wanshuo/Desktop/Master/2022:23_I_b/Collecting_Data/week5/corpora'\n",
    "\n",
    "# List to store filenames, original reviews, and processed reviews\n",
    "file_data = []\n",
    "\n",
    "# Regular expressions to match characters that are not letters, numbers, spaces and punctuation marks\n",
    "pattern = re.compile('[^a-zA-Z0-9\\s.,?!]')\n",
    "\n",
    "# Iterate through files in the specified folder\n",
    "for file_name in os.listdir(corpora_folder_path):\n",
    "    file_path = os.path.join(corpora_folder_path, file_name)\n",
    "    \n",
    "    # Ensure the current item is a file, not a folder\n",
    "    if os.path.isfile(file_path):\n",
    "        # Extract filenames and original reviews\n",
    "        filename = os.path.basename(file_path)\n",
    "        reviews_list = []\n",
    "        \n",
    "        # Read the file and extract reviews\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            review = ''\n",
    "            for line in file:\n",
    "                # Check for empty lines, add the previous review to the list, and clear the current review\n",
    "                if not line.strip():\n",
    "                    if review:\n",
    "                        reviews_list.append(review.strip())\n",
    "                        review = ''\n",
    "                else:\n",
    "                    review += line\n",
    "\n",
    "            # Process the last review (in case the file ends without an empty line)\n",
    "            if review:\n",
    "                reviews_list.append(review.strip())\n",
    "        \n",
    "        # Keep only English reviews using langdetect\n",
    "        english_reviews = [review for review in reviews_list if detect(review) == 'en']\n",
    "        \n",
    "        #Use regular expressions to remove non-letter, number, space and punctuation special characters\n",
    "        cleaned_reviews = [pattern.sub('', review) for review in english_reviews]\n",
    "        \n",
    "        # Unify reviews to lowercase using the lower() method\n",
    "        cleaned_reviews_lower = [review.lower() for review in cleaned_reviews]\n",
    "        \n",
    "        # Extend the file_data list with tuples containing filename, original review, and cleaned review\n",
    "        file_data.extend([(filename, document, text) for document, text in zip(reviews_list, cleaned_reviews_lower)])\n",
    "\n",
    "# Write the cleaned reviews to a CSV file\n",
    "csv_output_path = '/Users/wanshuo/Desktop/Master/2022:23_I_b/Collecting_Data/week5/reviews_output.csv'\n",
    "with open(csv_output_path, 'w', encoding='utf-8', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    # Write column names to the CSV file\n",
    "    csv_writer.writerow(['filename', 'document', 'text'])\n",
    "    \n",
    "    # Write filenames, original reviews and processed reviews\n",
    "    csv_writer.writerows(file_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df3eaa",
   "metadata": {},
   "source": [
    "To improve the quality of the corpus, invalid reviews can also be removed manually after removing non-letter, number, space and punctuation special characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e990ab9",
   "metadata": {},
   "source": [
    "# Text Enrichment with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61433bb",
   "metadata": {},
   "source": [
    "## Creating Doc Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7dc3933c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.8 MB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.25.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.59.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: setuptools in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.20.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (20.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/wanshuo/opt/Anaconda3/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Install English language model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4f78617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b22a2d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define your process_text function\n",
    "def process_text(text):\n",
    "    # Apply spaCy NLP pipeline to each document\n",
    "    doc = nlp(text)\n",
    "    return doc\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "csv_file_path = '/Users/wanshuo/Desktop/Master/2022:23_I_b/Collecting_Data/week5/reviews_output.csv' \n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Apply the process_text function to the 'text' column\n",
    "df['Doc'] = df['text'].apply(process_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d430e",
   "metadata": {},
   "source": [
    "## Text Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20199b39",
   "metadata": {},
   "source": [
    "### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a622faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve tokens from a doc object\n",
    "def get_token(doc):\n",
    "    return [(token.text) for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aad831fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>document</th>\n",
       "      <th>text</th>\n",
       "      <th>Doc</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>Where do I even start on this novel?it is famo...</td>\n",
       "      <td>where do i even start on this novel?it is famo...</td>\n",
       "      <td>(where, do, i, even, start, on, this, novel?it...</td>\n",
       "      <td>[where, do, i, even, start, on, this, novel?it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>Like many this was the first c-novel i ever re...</td>\n",
       "      <td>like many this was the first cnovel i ever rea...</td>\n",
       "      <td>(like, many, this, was, the, first, cnovel, i,...</td>\n",
       "      <td>[like, many, this, was, the, first, cnovel, i,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>It's Coiling Dragon, obviously you should read...</td>\n",
       "      <td>its coiling dragon, obviously you should read ...</td>\n",
       "      <td>(its, coiling, dragon, ,, obviously, you, shou...</td>\n",
       "      <td>[its, coiling, dragon, ,, obviously, you, shou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>This is the one that got me into the entire ge...</td>\n",
       "      <td>this is the one that got me into the entire ge...</td>\n",
       "      <td>(this, is, the, one, that, got, me, into, the,...</td>\n",
       "      <td>[this, is, the, one, that, got, me, into, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>Not a single fault of the series comes to mind...</td>\n",
       "      <td>not a single fault of the series comes to mind...</td>\n",
       "      <td>(not, a, single, fault, of, the, series, comes...</td>\n",
       "      <td>[not, a, single, fault, of, the, series, comes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     filename  \\\n",
       "0  reviews Coiling Dragon.txt   \n",
       "1  reviews Coiling Dragon.txt   \n",
       "2  reviews Coiling Dragon.txt   \n",
       "3  reviews Coiling Dragon.txt   \n",
       "4  reviews Coiling Dragon.txt   \n",
       "\n",
       "                                            document  \\\n",
       "0  Where do I even start on this novel?it is famo...   \n",
       "1  Like many this was the first c-novel i ever re...   \n",
       "2  It's Coiling Dragon, obviously you should read...   \n",
       "3  This is the one that got me into the entire ge...   \n",
       "4  Not a single fault of the series comes to mind...   \n",
       "\n",
       "                                                text  \\\n",
       "0  where do i even start on this novel?it is famo...   \n",
       "1  like many this was the first cnovel i ever rea...   \n",
       "2  its coiling dragon, obviously you should read ...   \n",
       "3  this is the one that got me into the entire ge...   \n",
       "4  not a single fault of the series comes to mind...   \n",
       "\n",
       "                                                 Doc  \\\n",
       "0  (where, do, i, even, start, on, this, novel?it...   \n",
       "1  (like, many, this, was, the, first, cnovel, i,...   \n",
       "2  (its, coiling, dragon, ,, obviously, you, shou...   \n",
       "3  (this, is, the, one, that, got, me, into, the,...   \n",
       "4  (not, a, single, fault, of, the, series, comes...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [where, do, i, even, start, on, this, novel?it...  \n",
       "1  [like, many, this, was, the, first, cnovel, i,...  \n",
       "2  [its, coiling, dragon, ,, obviously, you, shou...  \n",
       "3  [this, is, the, one, that, got, me, into, the,...  \n",
       "4  [not, a, single, fault, of, the, series, comes...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the token retrieval function on the doc objects in the dataframe\n",
    "df['Tokens'] = df['Doc'].apply(get_token)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bee30b4",
   "metadata": {},
   "source": [
    "### Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e300ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>document</th>\n",
       "      <th>text</th>\n",
       "      <th>Doc</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>Where do I even start on this novel?it is famo...</td>\n",
       "      <td>where do i even start on this novel?it is famo...</td>\n",
       "      <td>(where, do, i, even, start, on, this, novel?it...</td>\n",
       "      <td>[where, do, i, even, start, on, this, novel?it...</td>\n",
       "      <td>[where, do, I, even, start, on, this, novel?it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>Like many this was the first c-novel i ever re...</td>\n",
       "      <td>like many this was the first cnovel i ever rea...</td>\n",
       "      <td>(like, many, this, was, the, first, cnovel, i,...</td>\n",
       "      <td>[like, many, this, was, the, first, cnovel, i,...</td>\n",
       "      <td>[like, many, this, be, the, first, cnovel, I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>It's Coiling Dragon, obviously you should read...</td>\n",
       "      <td>its coiling dragon, obviously you should read ...</td>\n",
       "      <td>(its, coiling, dragon, ,, obviously, you, shou...</td>\n",
       "      <td>[its, coiling, dragon, ,, obviously, you, shou...</td>\n",
       "      <td>[its, coil, dragon, ,, obviously, you, should,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>This is the one that got me into the entire ge...</td>\n",
       "      <td>this is the one that got me into the entire ge...</td>\n",
       "      <td>(this, is, the, one, that, got, me, into, the,...</td>\n",
       "      <td>[this, is, the, one, that, got, me, into, the,...</td>\n",
       "      <td>[this, be, the, one, that, get, I, into, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>Not a single fault of the series comes to mind...</td>\n",
       "      <td>not a single fault of the series comes to mind...</td>\n",
       "      <td>(not, a, single, fault, of, the, series, comes...</td>\n",
       "      <td>[not, a, single, fault, of, the, series, comes...</td>\n",
       "      <td>[not, a, single, fault, of, the, series, come,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     filename  \\\n",
       "0  reviews Coiling Dragon.txt   \n",
       "1  reviews Coiling Dragon.txt   \n",
       "2  reviews Coiling Dragon.txt   \n",
       "3  reviews Coiling Dragon.txt   \n",
       "4  reviews Coiling Dragon.txt   \n",
       "\n",
       "                                            document  \\\n",
       "0  Where do I even start on this novel?it is famo...   \n",
       "1  Like many this was the first c-novel i ever re...   \n",
       "2  It's Coiling Dragon, obviously you should read...   \n",
       "3  This is the one that got me into the entire ge...   \n",
       "4  Not a single fault of the series comes to mind...   \n",
       "\n",
       "                                                text  \\\n",
       "0  where do i even start on this novel?it is famo...   \n",
       "1  like many this was the first cnovel i ever rea...   \n",
       "2  its coiling dragon, obviously you should read ...   \n",
       "3  this is the one that got me into the entire ge...   \n",
       "4  not a single fault of the series comes to mind...   \n",
       "\n",
       "                                                 Doc  \\\n",
       "0  (where, do, i, even, start, on, this, novel?it...   \n",
       "1  (like, many, this, was, the, first, cnovel, i,...   \n",
       "2  (its, coiling, dragon, ,, obviously, you, shou...   \n",
       "3  (this, is, the, one, that, got, me, into, the,...   \n",
       "4  (not, a, single, fault, of, the, series, comes...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [where, do, i, even, start, on, this, novel?it...   \n",
       "1  [like, many, this, was, the, first, cnovel, i,...   \n",
       "2  [its, coiling, dragon, ,, obviously, you, shou...   \n",
       "3  [this, is, the, one, that, got, me, into, the,...   \n",
       "4  [not, a, single, fault, of, the, series, comes...   \n",
       "\n",
       "                                              Lemmas  \n",
       "0  [where, do, I, even, start, on, this, novel?it...  \n",
       "1  [like, many, this, be, the, first, cnovel, I, ...  \n",
       "2  [its, coil, dragon, ,, obviously, you, should,...  \n",
       "3  [this, be, the, one, that, get, I, into, the, ...  \n",
       "4  [not, a, single, fault, of, the, series, come,...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to retrieve lemmas from a doc object\n",
    "def get_lemma(doc):\n",
    "    return [(token.lemma_) for token in doc]\n",
    "\n",
    "# Run the lemma retrieval function on the doc objects in the dataframe\n",
    "df['Lemmas'] = df['Doc'].apply(get_lemma)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873172c2",
   "metadata": {},
   "source": [
    "## Text Annotation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110e164",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c41525b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>document</th>\n",
       "      <th>text</th>\n",
       "      <th>Doc</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>Where do I even start on this novel?it is famo...</td>\n",
       "      <td>where do i even start on this novel?it is famo...</td>\n",
       "      <td>(where, do, i, even, start, on, this, novel?it...</td>\n",
       "      <td>[where, do, i, even, start, on, this, novel?it...</td>\n",
       "      <td>[where, do, I, even, start, on, this, novel?it...</td>\n",
       "      <td>[(SCONJ, WRB), (AUX, VBP), (PRON, PRP), (ADV, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>Like many this was the first c-novel i ever re...</td>\n",
       "      <td>like many this was the first cnovel i ever rea...</td>\n",
       "      <td>(like, many, this, was, the, first, cnovel, i,...</td>\n",
       "      <td>[like, many, this, was, the, first, cnovel, i,...</td>\n",
       "      <td>[like, many, this, be, the, first, cnovel, I, ...</td>\n",
       "      <td>[(INTJ, UH), (ADJ, JJ), (PRON, DT), (AUX, VBD)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>It's Coiling Dragon, obviously you should read...</td>\n",
       "      <td>its coiling dragon, obviously you should read ...</td>\n",
       "      <td>(its, coiling, dragon, ,, obviously, you, shou...</td>\n",
       "      <td>[its, coiling, dragon, ,, obviously, you, shou...</td>\n",
       "      <td>[its, coil, dragon, ,, obviously, you, should,...</td>\n",
       "      <td>[(PRON, PRP$), (VERB, VBG), (NOUN, NN), (PUNCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>This is the one that got me into the entire ge...</td>\n",
       "      <td>this is the one that got me into the entire ge...</td>\n",
       "      <td>(this, is, the, one, that, got, me, into, the,...</td>\n",
       "      <td>[this, is, the, one, that, got, me, into, the,...</td>\n",
       "      <td>[this, be, the, one, that, get, I, into, the, ...</td>\n",
       "      <td>[(PRON, DT), (AUX, VBZ), (DET, DT), (NOUN, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reviews Coiling Dragon.txt</td>\n",
       "      <td>Not a single fault of the series comes to mind...</td>\n",
       "      <td>not a single fault of the series comes to mind...</td>\n",
       "      <td>(not, a, single, fault, of, the, series, comes...</td>\n",
       "      <td>[not, a, single, fault, of, the, series, comes...</td>\n",
       "      <td>[not, a, single, fault, of, the, series, come,...</td>\n",
       "      <td>[(PART, RB), (DET, DT), (ADJ, JJ), (NOUN, NN),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     filename  \\\n",
       "0  reviews Coiling Dragon.txt   \n",
       "1  reviews Coiling Dragon.txt   \n",
       "2  reviews Coiling Dragon.txt   \n",
       "3  reviews Coiling Dragon.txt   \n",
       "4  reviews Coiling Dragon.txt   \n",
       "\n",
       "                                            document  \\\n",
       "0  Where do I even start on this novel?it is famo...   \n",
       "1  Like many this was the first c-novel i ever re...   \n",
       "2  It's Coiling Dragon, obviously you should read...   \n",
       "3  This is the one that got me into the entire ge...   \n",
       "4  Not a single fault of the series comes to mind...   \n",
       "\n",
       "                                                text  \\\n",
       "0  where do i even start on this novel?it is famo...   \n",
       "1  like many this was the first cnovel i ever rea...   \n",
       "2  its coiling dragon, obviously you should read ...   \n",
       "3  this is the one that got me into the entire ge...   \n",
       "4  not a single fault of the series comes to mind...   \n",
       "\n",
       "                                                 Doc  \\\n",
       "0  (where, do, i, even, start, on, this, novel?it...   \n",
       "1  (like, many, this, was, the, first, cnovel, i,...   \n",
       "2  (its, coiling, dragon, ,, obviously, you, shou...   \n",
       "3  (this, is, the, one, that, got, me, into, the,...   \n",
       "4  (not, a, single, fault, of, the, series, comes...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [where, do, i, even, start, on, this, novel?it...   \n",
       "1  [like, many, this, was, the, first, cnovel, i,...   \n",
       "2  [its, coiling, dragon, ,, obviously, you, shou...   \n",
       "3  [this, is, the, one, that, got, me, into, the,...   \n",
       "4  [not, a, single, fault, of, the, series, comes...   \n",
       "\n",
       "                                              Lemmas  \\\n",
       "0  [where, do, I, even, start, on, this, novel?it...   \n",
       "1  [like, many, this, be, the, first, cnovel, I, ...   \n",
       "2  [its, coil, dragon, ,, obviously, you, should,...   \n",
       "3  [this, be, the, one, that, get, I, into, the, ...   \n",
       "4  [not, a, single, fault, of, the, series, come,...   \n",
       "\n",
       "                                                 POS  \n",
       "0  [(SCONJ, WRB), (AUX, VBP), (PRON, PRP), (ADV, ...  \n",
       "1  [(INTJ, UH), (ADJ, JJ), (PRON, DT), (AUX, VBD)...  \n",
       "2  [(PRON, PRP$), (VERB, VBG), (NOUN, NN), (PUNCT...  \n",
       "3  [(PRON, DT), (AUX, VBZ), (DET, DT), (NOUN, NN)...  \n",
       "4  [(PART, RB), (DET, DT), (ADJ, JJ), (NOUN, NN),...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to retrieve lemmas from a doc object\n",
    "def get_pos(doc):\n",
    "    #Return the coarse- and fine-grained part of speech text for each token in the doc\n",
    "    return [(token.pos_, token.tag_) for token in doc]\n",
    "\n",
    "# Define a function to retrieve parts of speech from a doc object\n",
    "df['POS'] = df['Doc'].apply(get_pos)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40208c97",
   "metadata": {},
   "source": [
    "## Download Enriched Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "419dd486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this step only to save  csv to your computer's working directory\n",
    "df.to_csv('reviews_with_spaCy_tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a54d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
